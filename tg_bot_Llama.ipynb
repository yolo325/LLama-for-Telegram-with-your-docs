{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_HfpwgKjJ545"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk8AiXeI1z4A"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade setuptools wheel\n",
        "!pip install pyTelegramBotAPI\n",
        "!pip install openai langchain faiss-cpu tiktoken\n",
        "!pip install schedule\n",
        "!pip install config\n",
        "!pip install python-dotenv\n",
        "!pip install gTTs\n",
        "!pip install celery\n",
        "!pip -q install langchain openai tiktoken PyPDF2 faiss-cpu\n",
        "!pip install llama-cpp-python\n",
        "\n",
        "!pip install ipykernel\n",
        "!python -m ipykernel install\n",
        "!conda install notebook ipykernel\n",
        "!ipython kernelspec install-self"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured\n",
        "!pip uninstall pdfminer\n",
        "!pip install opencv-python\n",
        "!pip install pdfminer.six\n",
        "!pip install langchain openai chromadb tiktoken pypdf unstructured pdf2image\n",
        "!pip install unstructured.pytesseract\n",
        "!pip install unstructured_inference\n",
        "!pip install chromadb\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch==1.11.0+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!pip install sentence-transformers\n",
        "!pip install accelerate\n",
        "!pip install optimum\n",
        "!pip install auto-gptq\n",
        "!pip install pikepdf\n",
        "!pip install pillow-heif\n",
        "!pip install -U langchain-community\n",
        "!sudo apt-get install -y poppler-utils\n",
        "!pip install --upgrade pdf2image"
      ],
      "metadata": {
        "id": "bMgB4bAn3Xhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "import random\n",
        "import logging\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import queue\n",
        "import threading\n",
        "\n",
        "from llama_cpp import Llama\n",
        "import telebot\n",
        "from telebot import types\n",
        "import logging\n",
        "import sqlite3\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.document_loaders.pdf import UnstructuredPDFLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "from pdf2image import convert_from_path\n",
        "from ast import literal_eval"
      ],
      "metadata": {
        "id": "tyM8c5CY5Bem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "eEmRW8DY5gvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GgHrUn-X5itC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1njZd0ZF3VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing your documents"
      ],
      "metadata": {
        "id": "fH38mOz9F3P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredPDFLoader(\"/content/sample_data/your_data_here_pdf\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "eK-t_0hR5ipW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_splitter = RecursiveCharacterTextSplitter(chunk_size = 1024, chunk_overlap = 64)\n",
        "texts = txt_splitter.split_documents(docs)\n",
        "len(texts)"
      ],
      "metadata": {
        "id": "MLK12y3k7Qit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name = \"thenlper/gte-large\",\n",
        "    model_kwargs = {\"device\": \"cuda\"},\n",
        "    encode_kwargs = {\"normalize_embeddings\": True},\n",
        ")"
      ],
      "metadata": {
        "id": "PzOWyVDq7QXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(len(query_result))"
      ],
      "metadata": {
        "id": "FkJwSbOZ7XGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
        "results = db.similarity_search(\"an example of essay\", k = 2)\n",
        "len(results)"
      ],
      "metadata": {
        "id": "b5BpgRZ37ZD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#end of the docs part"
      ],
      "metadata": {
        "id": "CDN19noZ7g5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xD4Z71c67g2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN = 'your_token'"
      ],
      "metadata": {
        "id": "YDpRRMXm7gul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)"
      ],
      "metadata": {
        "id": "8GPEPbXj7i1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")"
      ],
      "metadata": {
        "id": "SCCaS3Dk7lN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "ej8DR9S5AxQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatHistoryDB:\n",
        "    def __init__(self, database_name):\n",
        "        self.db_name = database_name\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.create_tables()\n",
        "\n",
        "    def create_tables(self):\n",
        "        conn = sqlite3.connect(self.db_name)\n",
        "        self.logger.info(f\"Database {self.db_name} opened\")\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create the message table\n",
        "        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n",
        "            message_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            chat_id INTEGER NOT NULL,\n",
        "            user_prompt TEXT NOT NULL,\n",
        "            answer TEXT NOT NULL,\n",
        "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "            deleted INTEGER DEFAULT 0\n",
        "        );''')\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def insert_message(self, chat_id, user_prompt, answer):\n",
        "        conn = sqlite3.connect(self.db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"INSERT INTO messages (chat_id, user_prompt, answer) VALUES (?, ?, ?)\",\n",
        "                       (chat_id, user_prompt, answer))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def get_chat_messages(self, chat_id, limit=5):\n",
        "        conn = sqlite3.connect(self.db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT user_prompt, answer FROM messages WHERE chat_id = ? AND deleted = 0 LIMIT ?\",\n",
        "                       (chat_id, limit))\n",
        "        results = cursor.fetchall()\n",
        "        conn.close()\n",
        "        return results\n",
        "\n",
        "    def delete_all_history(self, chat_id):\n",
        "        conn = sqlite3.connect(self.db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"UPDATE messages SET deleted = 1 WHERE chat_id = ?\", (chat_id,))\n",
        "        conn.commit()\n",
        "        conn.close()"
      ],
      "metadata": {
        "id": "fH_f4L2uAzGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='LLaMa telegram bot')\n",
        "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
        "parser.add_argument('-t', '--threads', default=4, type=int, help='Number of threads to use')\n",
        "parser.add_argument('--max-tokens', default=128, type=int, help='The maximum number of tokens to generate')\n",
        "parser.add_argument('--enable-history', action='store_true', help='Simulate memory in a chatbot')\n",
        "parser.add_argument('--skip-init-prompt', action='store_true', help='Skip the initial prompt (faster startup)')\n",
        "parser.add_argument('--debug', action='store_true', help='Enable debug logging')\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "1yi1NlBOdeoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\n",
        "model_basename = \"codellama-13b-python.Q5_K_M.gguf\"\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "taeymuPjdfS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_level = logging.DEBUG if args.debug else logging.INFO\n",
        "log_format = '%(asctime)s [%(levelname)s] %(message)s'\n",
        "date_format = '%Y-%m-%d %H:%M:%S'\n",
        "\n",
        "logging.basicConfig(level=log_level, format=log_format, datefmt=date_format)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "bot = telebot.TeleBot(TOKEN)\n",
        "seed = random.randint(1, sys.maxsize)\n",
        "\n",
        "try:\n",
        "    llama = Llama(model_path=model_path, n_ctx=512, n_gqa=8, seed=seed, n_threads=args.threads, verbose=True)\n",
        "except ValueError as e:\n",
        "    logger.error(\"Error while initializing LLaMa: \"+str(e))\n",
        "    sys.exit(1)\n",
        "\n",
        "historyDb = ChatHistoryDB(\"chat.db\")\n",
        "job_queue = queue.Queue()\n",
        "\n",
        "init_prompt = \"Write a short response that appropriately completes\" \\\n",
        "              \"the request.\"\n",
        "q_prompt = \"### Instruction:\"\n",
        "a_prompt = \"### Response:\"\n",
        "\n",
        "\n",
        "def process_job(job):\n",
        "    def generate_text(user_prompt, max_tokens=args.max_tokens, stream=False, custom_prompt=False, chat_id=None,\n",
        "                      history= args.enable_history):\n",
        "        if args.skip_init_prompt:\n",
        "            prompt = f\"{q_prompt}{user_prompt}\\n{a_prompt}\"\n",
        "        else:\n",
        "            prompt = f\"{init_prompt}\\n{q_prompt} {user_prompt}\\n{a_prompt}\"\n",
        "        if custom_prompt:\n",
        "            prompt = user_prompt\n",
        "        if history and chat_id:\n",
        "            prompt = get_last_messages(chat_id) + prompt\n",
        "\n",
        "        logger.debug(f\"Generation for: {prompt}\")\n",
        "\n",
        "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "        def create_chain(_retriever):\n",
        "            qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=_retriever, memory=memory, verbose=False)\n",
        "            return qa_chain\n",
        "\n",
        "        llm_chain = create_chain(db.as_retriever(search_kwargs={\"k\":2}))\n",
        "\n",
        "        if stream and max_tokens > 2048:\n",
        "            logger.warning(\"This is likely to exceed 4096 characters, which would not fit into one stream message\")\n",
        "        nswr = llm_chain.run(prompt)\n",
        "        print(nswr)\n",
        "        helpful_answer_index = nswr.find(\"Helpful Answer:\")\n",
        "        if helpful_answer_index != -1:\n",
        "          start_index = helpful_answer_index + len(\"Helpful Answer:\") + 1\n",
        "          response_text = nswr[start_index:].strip()\n",
        "        return response_text\n",
        "\n",
        "    user_prompt = job[0]\n",
        "    chat_id = job[1]\n",
        "    msg = job[2]\n",
        "    custom_prompt = job[3]\n",
        "\n",
        "    try:\n",
        "        bot.edit_message_text(chat_id=msg.chat.id, text=\"Started to generate text for you...\",\n",
        "                              message_id=msg.message_id)\n",
        "        logger.info(f\"Generating text for user {msg.chat.username}\")\n",
        "\n",
        "        if custom_prompt:\n",
        "            json_obj = generate_text(user_prompt, chat_id=chat_id, stream=False,\n",
        "                                     custom_prompt=True)\n",
        "        else:\n",
        "            json_obj = generate_text(user_prompt, chat_id=chat_id, stream=False)\n",
        "        output = json_obj\n",
        "\n",
        "        logger.debug(json.dumps(json_obj, indent=2))\n",
        "        text_to_user = output\n",
        "\n",
        "        send_by_chunks(msg, text_to_user)\n",
        "        logger.info(f\"Sent to user {msg.chat.username}: {text_to_user}\")\n",
        "\n",
        "        if args.enable_history:\n",
        "            historyDb.insert_message(chat_id, user_prompt, output)\n",
        "        bot.delete_message(msg.chat.id, msg.message_id)  # delete 'please wait a moment'\n",
        "    except OSError as e:\n",
        "        bot.reply_to(msg, f\"OSError: {e}\")\n",
        "    except Exception as e:\n",
        "        bot.reply_to(msg, f\"Error: {e}\")\n",
        "\n",
        "\n",
        "def process_queue():\n",
        "    while True:\n",
        "        try:\n",
        "            job = job_queue.get()\n",
        "            start_time = time.time()\n",
        "            process_job(job)\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            logger.info(f\"Job processed in {elapsed_time:02f} seconds\")\n",
        "            job_queue.task_done()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "def send_by_chunks(message, text, **kwargs):\n",
        "    if len(text) < 5:\n",
        "        logger.error(\"Message is empty or too short\")\n",
        "    if len(text) <= 4096:\n",
        "        bot.send_message(message.chat.id, text, **kwargs)\n",
        "    else:\n",
        "        chunks = []\n",
        "        while len(text) > 0:\n",
        "            chunk = text[:4096]\n",
        "            chunks.append(chunk)\n",
        "            text = text[4096:]\n",
        "        for chunk in chunks:\n",
        "            bot.reply_to(message, chunk, **kwargs)\n",
        "\n",
        "\n",
        "def get_last_messages(chat_id):\n",
        "    messages = historyDb.get_chat_messages(chat_id)\n",
        "    history = \"\"\n",
        "    for user_prompt, answer in messages:\n",
        "        history += f\"{q_prompt} {user_prompt}\\n {a_prompt} {answer}\\n\"\n",
        "    return history\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['history'])\n",
        "def history_command(message):\n",
        "    keyboard = types.InlineKeyboardMarkup()\n",
        "    button = types.InlineKeyboardButton(text=\"Delete history\", callback_data=\"remove_history\")\n",
        "    keyboard.add(button)\n",
        "    history = get_last_messages(message.chat.id)\n",
        "    if history == \"\":\n",
        "        history = \"You have no history with the bot\"\n",
        "    bot.send_message(message.chat.id, text=history, reply_markup=keyboard)\n",
        "\n",
        "\n",
        "@bot.callback_query_handler(func=lambda call: call.data == 'remove_history')\n",
        "def send_message_callback(call):\n",
        "    historyDb.delete_all_history(call.message.chat.id)\n",
        "    bot.send_message(call.message.chat.id, \"Chat history successfully forgotten\")\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['start', 'help'])\n",
        "def start_command(message):\n",
        "    bot.reply_to(message, \"Hello. This is chatGPT bot based on LLaMa.\\nUsage:\"\n",
        "                          \"\\n\\n<text> - Q&A mode.\\n\"\n",
        "                          \"/raw <prompt> - Use your own prompt.\\n\"\n",
        "                          \"/history - show history and delete it\\n\\n\"\n",
        "                          f\"Current model: {model_basename}\")\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['raw'])\n",
        "def raw_command(message):\n",
        "    user_prompt = message.text.replace(\"/raw \", '', 1)\n",
        "\n",
        "    msg = bot.reply_to(message, f\"Please wait a moment. Current queue: {job_queue.qsize()}\")\n",
        "    bot.send_chat_action(chat_id=message.chat.id, action='typing')\n",
        "    job_queue.put((user_prompt, message.chat.id, msg, True))\n",
        "\n",
        "\n",
        "@bot.message_handler(func=lambda message: True)\n",
        "def main(message):\n",
        "    if message.text.startswith(\"/\"):\n",
        "        bot.reply_to(message, \"Wrong command\")\n",
        "        return\n",
        "    user_prompt = message.text\n",
        "    msg = bot.reply_to(message, f\"Please wait a moment. Current queue: {job_queue.qsize()}\")\n",
        "    bot.send_chat_action(chat_id=message.chat.id, action='typing')\n",
        "    job_queue.put((user_prompt, message.chat.id, msg, False))\n",
        "    logger.info(\"Added a new task from user: %s (%s), text: %s\",  message.chat.username,  message.chat.id, user_prompt)\n",
        "\n",
        "\n",
        "t = threading.Thread(target=process_queue)\n",
        "t.daemon = True\n",
        "t.start()\n",
        "\n",
        "bot.infinity_polling()"
      ],
      "metadata": {
        "id": "ObEQBepUdiyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "SvQrE96bjfwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXgsUBj0jhuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}